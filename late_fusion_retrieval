import numpy as np
from unimodal import UnimodalRetrievalSystem, MODALITIES


class MultimodalRetrievalSystem(UnimodalRetrievalSystem):
    """
    Multimodal retrieval with late fusion 

    Trying different fusion and weighting strategies
    fusion:
      - "rrf": Reciprocal Rank Fusion (rank-based, robust, no normalization needed)
      - "norm_sum": normalize per modality then (optionally weighted) sum

    weighting (only needed for norm_sum as this is a weighted sum):
      - "equal": equal weights
      - "auto": entropy-based confidence heuristic
    """

    @staticmethod
    def _zscore(x, eps=1e-12):
        x = np.asarray(x, dtype=float)
        return (x - x.mean()) / (x.std() + eps)

    @staticmethod
    def _minmax(x, eps=1e-12):
        x = np.asarray(x, dtype=float)
        mn, mx = float(x.min()), float(x.max())
        return (x - mn) / (mx - mn + eps)

    @staticmethod
    def _softmax(x):
        x = x - np.max(x)
        e = np.exp(x)
        return e / np.sum(e)

    def _auto_weights(self, scores_by_modality, topL=200, alpha=2.0, eps=1e-12):
        """
        Agreement-based confidence:
        - For each modality, take TopL ids.
        - Confidence = average Jaccard overlap with other modalities' TopL sets. (measures agreement between rankings and uses that as "confidence")
        - Weights = soft-normalized confidences^alpha.
        - Scaling parameter alpha

        with alpha > 1 higher confidence scores are amplified significantly more than lower ones
        """

        # just one modality, use weight of 1
        M = len(scores_by_modality)
        if M < 2:
            return np.array([1.0])

        # build a top L set for each modality
        top_sets = []
        for s in scores_by_modality:
            s = np.asarray(s, dtype=float)
            L = min(topL, s.size)
            top_idx = np.argpartition(-s, kth=L-1)[:L]  # indices of TopL (unordered)
            top_sets.append(set(top_idx.tolist()))

        # compute confidence using jaquard overlaüA
        conf = np.zeros(M, dtype=float)
        for m in range(M):
            overlaps = []
            for j in range(M):
                if j == m:
                    continue
                A, B = top_sets[m], top_sets[j]
                inter = len(A & B)
                union = len(A | B) + eps
                overlaps.append(inter / union)
            conf[m] = float(np.mean(overlaps)) if overlaps else 0.0

        # if we have very disjoint topL sets (little to no agreement) we use equal weights again
        if np.all(conf <= eps):
            w = np.ones(M, dtype=float)
            return w / w.sum()

        conf = np.maximum(conf, 0.0) ** alpha
        return conf / (conf.sum() + eps)


    def retrieve_multimodal(
        self,
        query_id: str,
        modalities,
        k_neighbors: int,
        fusion: str = "rrf",        # rrf or norm_sum
        norm: str = "zscore",       # zscore or minmax (only for norm_sum)
        weighting: str = "equal",   # equal or auto (only for norm_sum)
        rrf_k: int = 60, # common value for that constant
    ):
        # sanitiy checks
        if not isinstance(modalities, (list, tuple)):
            raise TypeError("modalities must be a list or tuple.")

        if len(modalities) < 2:
            raise ValueError("Need at least 2 modalities for multimodal retrieval.")

        for m in modalities:
            if m not in MODALITIES or m not in self.data:
                raise ValueError(f"Invalid modality '{m}'. Available: {list(self.data.keys())}")

        if k_neighbors <= 0:
            raise ValueError("k_neighbors must be > 0")

        # compute scores per modality 

        # get ids present in all modalities (intersection of list of sets, one per modality)
        id_sets = [set(self.data[m][1]) for m in modalities]
        common_ids = set.intersection(*id_sets)
        
        # query track must be in all modalities
        if query_id not in common_ids:
            raise ValueError(f"Query id {query_id} missing in at least one selected modality.")

        # list of common IDs in a fixed, reproducible order
        ref_index_to_id = self.data[modalities[0]][1]
        common_ids_ordered = [tid for tid in ref_index_to_id if tid in common_ids]

        # compute similarity scores for each modality, aligned to that order
        scores_by_modality = []
        for m in modalities:
            matrix, index_to_id, id_to_index = self.data[m]
            q_idx = id_to_index[query_id]
            q_vec = matrix[q_idx]

            # compute similarities against all common tracks
            idxs = np.array([id_to_index[tid] for tid in common_ids_ordered], dtype=int)
            s = matrix[idxs] @ q_vec # since our unimodal preprocessing normalizes vectors to unit length -> dot product = cosine similarity
            scores_by_modality.append(s)

        # remove query track
        q_pos = common_ids_ordered.index(query_id)
        candidate_ids = common_ids_ordered[:q_pos] + common_ids_ordered[q_pos + 1:]
        scores_by_modality = [np.delete(s, q_pos) for s in scores_by_modality]

        # try out different fusion strategies 
        if fusion == "rrf": # Reciprocal Rank Fusion
            fused = np.zeros(len(candidate_ids), dtype=float)
            for s in scores_by_modality:
                order = np.argsort(-s)  # descending
                ranks = np.empty_like(order)
                ranks[order] = np.arange(1, len(s) + 1)  # 1..N
                fused += 1.0 / (rrf_k + ranks.astype(float))

        # this is similar to the weighted average from before just with automatically computed weights
        elif fusion == "norm_sum": #  score-based combination technique, involves combining the raw scores after they have been scaled to a consistent range
            # need a normalization strategy to make them comparable
            if norm == "zscore":
                normed = [self._zscore(s) for s in scores_by_modality]
            elif norm == "minmax":
                normed = [self._minmax(s) for s in scores_by_modality]
            else:
                raise ValueError("norm must be 'zscore' or 'minmax' (others are not implemented yet)")

            # choose weights
            if weighting == "equal": # very simple default (each item is equally important)
                w = np.ones(len(modalities), dtype=float)
                w /= w.sum()
            elif weighting == "auto":
                w = self._auto_weights(scores_by_modality)
            else:
                raise ValueError("weighting must be 'equal' or 'auto'")

            # sum the normalized scores multiplied with the weights
            fused = np.zeros(len(candidate_ids), dtype=float)
            for wi, si in zip(w, normed):
                fused += wi * si

        else:
            raise ValueError("fusion must be 'rrf' or 'norm_sum'")

        # get top k results
        k = min(k_neighbors, len(candidate_ids))
        top_idx = np.argpartition(-fused, kth=k - 1)[:k]
        top_idx = top_idx[np.argsort(-fused[top_idx])]

        top_ids = [candidate_ids[i] for i in top_idx]
        top_scores = [float(fused[i]) for i in top_idx]
        return top_ids, top_scores


if __name__ == "__main__":
    data_root = "./data"
    mm = MultimodalRetrievalSystem(data_root)

    query_id = "NDroPROgWm3jBxjH"
    modalities = ["audio", "lyrics", "video"]
    k = 5

    print("\n=== MULTIMODAL RETRIEVAL (NEW FUSION) ===")
    print("Query:", query_id)
    print("Modalities:", modalities)
    print("k:", k)

    print("\n--- Fusion: RRF (default) ---")
    ids_rrf, scores_rrf = mm.retrieve_multimodal(
        query_id=query_id,
        modalities=modalities,
        k_neighbors=k,
        fusion="rrf",
        rrf_k=60,
    )
    print("Retrieved IDs:", ids_rrf)
    print("Fused Scores:", scores_rrf)

    print("\n--- Fusion: norm_sum (zscore + equal) ---")
    ids_ns_eq, scores_ns_eq = mm.retrieve_multimodal(
        query_id=query_id,
        modalities=modalities,
        k_neighbors=k,
        fusion="norm_sum",
        norm="zscore",
        weighting="equal",
    )
    print("Retrieved IDs:", ids_ns_eq)
    print("Fused Scores:", scores_ns_eq)

    print("\n--- Fusion: norm_sum (zscore + auto) ---")
    ids_ns_auto, scores_ns_auto = mm.retrieve_multimodal(
        query_id=query_id,
        modalities=modalities,
        k_neighbors=k,
        fusion="norm_sum",
        norm="zscore",
        weighting="auto",
    )
    print("Retrieved IDs:", ids_ns_auto)
    print("Fused Scores:", scores_ns_auto)

    print("\n=== OVERLAP ===")
    print("RRF ∩ norm_sum(equal):", sorted(set(ids_rrf) & set(ids_ns_eq)))
    print("RRF ∩ norm_sum(auto): ", sorted(set(ids_rrf) & set(ids_ns_auto)))

    # TODO: Later check which strategy is the most promising one

